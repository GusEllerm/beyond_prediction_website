<p>Beyond Prediction: MBIE Annual Report 2023-4</p><p>Public statement</p><p>Our overall aim is to develop new data science methods that help to <strong>improve the process of conducting and reporting research</strong>, and to use developments in data science to <strong>improve our national capability in genomics and biology research</strong>, thus helping New Zealand to remain competitive in these two important areas.</p><p>One of our major project goals is to help make research more transparent and trustable. Over the past year, we have developed a scientific claim verification methodology and associated tool allows questions to be posed about the accuracy and supporting evidence for any claim made against the scientific literature. An associated web application allows claims to be evaluated in real time against established scientific literature. This kind of tool helps to <strong>empower non-scientists to ask specific questions about scientific findings and evidence</strong>, and to explore for themselves the peer-reviewed evidence that supports, and refutes, any science claim.</p><p>Another major achievement over the past year is the use of Digital Twin technology to <strong>create a genomic digital twin of a threatened native species</strong>, the Hihi (stitchbird). This digital twin allows us to model with precision how the genomics of the current Hihi population will affect its ability to survive and thrive in a changing world. Such methods allow us to better understand how our precious flora and fauna will react in the future to changes in their environment, so we can understand associated risks before changes occur and plan for better mitigation.</p><p>Collaborations within our team are now leading to additional grant proposals in the areas of AI and genomics, and we have strengthened our collaborations with other research partners based in Aotearoa (notably <strong>ESR and Genomics Aotearoa</strong>) along with several prestigious overseas collaborations, including the <strong>Allen AI Institute</strong> and <strong>Globus Labs.</strong></p><p>We sponsored and helped to deliver a significant national outreach &amp; training event (ResBaz 2024: <a href="https://resbaz.auckland.ac.nz/schedule/">https://resbaz.auckland.ac.nz/schedule/</a>) enabling over <strong>1000 researchers to learn new data science skills</strong> that they can apply to their own research.  </p><p><strong>Strategic Intent</strong></p><p>Our strategic direction focuses on developing new data science methods that actually improve the conduct of research, and to demonstrate some of these improvements in the life science, specifically: Biology, Genomics, Population Health and Ecology.</p><p>We provide a high-level summary of our work towards these strategic directions below.</p><p>Our <strong><em>Live Data Science</em></strong> initiative aims at <strong>closing the gap between the ‘doing’ of science and the ‘publishing’ of science.</strong> Why not instead create workflows that update ‘live’ documents in real time, whenever new data becomes available? In an era when data are ubiquitous and regularly updated, the lag in writing and publishing science—whether in academic journals or in reports for the government—becomes restrictive and is a source of great inefficiency.</p><p>Our <strong>Transparent Data Science</strong> initiative concentrates on the challenge of improving the trustworthiness and reproducibility of data-intensive research, providing tools to question scientific claims and query research processes. This opens up research to more scrutiny, which helps to improve trust, and offers new avenues for public engagement.</p><p>Our <strong>Meta Data Science initiative</strong> recognises that the major bottleneck in the widespread adoption of data science methods is still the ingestion of data, and all that this implies about creating a common schema. We are developing new methods (including AI methods) to automate large parts of this process, so that data that was never designed to be integrated, and is perhaps even completely undocumented can nevertheless be harmonised for use in new and unanticipated situations. This work promises real benefits for many sectors, including government and industry, in terms of both cost and time savings.</p><p><strong>Applications of Data Science in the Life Sciences</strong></p><p>Our work here tackles a multitude of longstanding analytical challenges and problems in the Life Sciences, including: (i) new techniques to improve the accuracy of phylodynamic modeling, with applications into infectious diseases, (ii) modelling the entire genone of a species and using this to anticipate adaptation capacity to a changing environment (iii) understanding the complex relationships between population genomics and common illnesses, to better target health interventions and (iv) better understanding and thus ability to control the spread of infectious diseases through susceptible populations.</p>

<ol type="1"><li>

<p><strong>Progress</strong></p>

</li></ol>

<p><strong>Research articles that write and update themselves</strong></p><p><strong>(Live and Transparent Data Science)</strong></p><p>Over the past 12 months, our LivePublication project has made strides in technical development whilst increasing connections to a variety of external groups and sharing the work at a variety of international meetings. Some key highlights include the following. In October 2023 PhD student, Gus Ellerm, attended and presented the paper “LivePublication: The Science Workflow Creates and Updates the Publication” at the international eScience 2023 conference in Cyprus. As an outcome of ongoing collaboration with the University of Chicago and Globus Labs, in May Gus was invited to speak at the industry Conference “Globus World” in Chicago, USA, as the closing presentation. He was also invited to give a talk at CWL (Common Workflow Language) Con based in Amsterdam and to be on a panel for the session on provenance in workflow systems. Throughout 2023 – 2024 we attended multiple RO-Crate community meetings throughout the year, and continue to build ties with the community, developing our data model with their input. </p><p>In terms of key technical development, we have developed a novel way of managing provenance information within distributed and federated systems, in order to better integrate scientific reports articles with distributed scientific workflows. We built this based on the RO-Crate standard (an active international community effort to develop better tooling to package research data: <a href="https://www.researchobject.org/ro-crate/">https://www.researchobject.org/ro-crate/</a>)) and extending the provenance run crate profile, developed by RO-Crate community members, for specific application in distributed and federated compute environments. The resulting model of <em><strong>workflow provenance</strong></em> helps enables us to capture full details of experimental workflows, even when those workflows are physically distributed across multiple sites or systems. This can be used by other researchers to better understand and replicate experiments—thus moves us much closer to our project goal of reproducible and understandable science.</p><p>We are now working to provide fine-grain integration of the computational workflow with science publications. Using this technology, key sections of a scientific document essentially ‘write themselves’, by translating aspects of the computational workflow directly into human-readable text. So for example, descriptions of experiments can be ‘harvested’ from the workflow provenance representation described above, and, with the help of Generative AI, can be turned into accurate prose, understandable by humans. Similarly, results (such as tables, figures) can be directly input from the data produced when the workflow is executed. Furthermore, these aspects of the research lifecycle remain live-connected to the research article: so if new data is available, the results can be automatically updated, or the workflow is changed, then the description in the article is also modified to stay consistent with these changes. Thus, a report on (say) an invasive species, or new viral pandemic can be created in such a way that it automatically updates in response to new methods and data.</p><p>This satisfies a major goal of our research, and one that we now have met in terms of developing the underlying theory and technology. We published some of our progress late last year, at the premier eScience conference (IEEE eScience). Over the remaining two years of this grant, we will roll this technology out into publishing platforms, one via our research partner <em>Stencila</em>, another via <em>GigaScience</em>, an international journal publisher that is part of Oxford University Press<em>. Relatedly, Stencila</em> now has integrations with Large Language Models (LLMs) enabling scientists to get assistance with writing both code and prose within their live publications. <em>Stencila's</em> integration is unique in that it records, to the character level, the provenance of content including the version or LLM used and the user's prompts. Ultimately, the aim is to make scientists more productive at doing science whilst bringing increased transparency and assurances to fellow scientists that their research outputs are genuine and novel.</p><p>The <em>Stencila</em> command line tool has been released as a beta version and a VSCode extension will be published to the VSCode Marketplace within the next month. The extension allows users to write live publications using standard academic authoring tools such as Markdown, MyST Markdown, or Jupyter Notebooks, all with the assistance of LLMs, from within VSCode.</p><p><strong>Evaluating the trustworthiness of scientific claims</strong></p><p><strong>(Trusted Data Science)</strong></p><p>One of our major project goals is to help make research more transparent and trustable. Over the past year, we have developed a scientific claim verification methodology that grounds large language models with retrieval-augmented generation, enhancing the transparency and trustworthiness in scientific claims. This tool allows questions to be posed about the accuracy and supporting evidence for any claim made against the scientific literature. An associated web application <strong>allows claims to be evaluated in real time against established scientific literature</strong>.</p><p>We know that Generative AI can provide summaries of published research far more quickly than humans can. Yet, these GenAI summaries are often unreliable, containing hallucinations and misleading information. We have been working to remove these sources of error. Our new methodology includes an inference method integrating multiple viewpoints from the literature to assess contradictory arguments and implicit assumptions. It distils information from diverse scientific abstracts, incorporates faithful reasoning into large language models, and generates verdict labels weighted by the <em>quality &amp; reputation metrics</em> of the contributing scientific articles.</p><p> </p><p>This advancement holds the potential to significantly benefit the scientific community by improving the accuracy and confidence in claim verification. It also aims to support public health and safety efforts by mitigating the spread of misinformation. Furthermore, the general public stands to gain from increased transparency and traceability, which empower them to make informed choices based on verified scientific evidence. These contributions are crucial for upholding the integrity and trustworthiness of scientific communication, especially during critical situations such as pandemics, climate change impacts, the likely impacts of GM agriculture, etc, as well as helping to debunk ‘bogus science’ that is designed to manipulate public behaviour. See later section on Partnerships for more information on who we are working with.</p><p><strong>Genomic Digital Twins for Threatened Species</strong></p><p><strong>(Advanced Data Science Methods in Biology &amp; Ecology)</strong></p><p>In the last year we have developed a new simulation model to test co-evolutionary models of hosts with their microbial communities. Our findings show that that multiple generations of microbes within a host generation improves both host and microbe fitness in a changing environment (compared to only one microbe generation per host generation).</p><p>We have also created the tools to allow us to study genetic recombination more closely. We have modelled the recombination landscape in <em>Hihi</em> (stitchbird; Notiomystis cincta). Recombination is a fundamental evolutionary process that creates new combinations of variants, with important implications for the adaptive potential of species. We have further developed our population simulation model of the Hihi metapopulation, which includes a deep literature search to document all previous translocations for the species in order to create a realistic 'genomic digital twin' of the Hihi. We are currently using this digital twin to test the impact of possible management decisions on adaptive potential for this taonga threatened species. All in the team continue to be focused on producing reproducible, reusable code and simulations, shared via GitHub.</p><p><strong>Improving the accuracy of phylogenetic models</strong></p><p><strong>(Advanced Data Science Methods in Genomics)</strong></p><p>Our work is focused on developing new data science methods for scaling up state-of-the-art analytical techniques to the very large datasets we encounter in genomics and phylogenetics. In doing so, we improve our understanding of genomics and improve predictability of key genomic outcomes from complex, real-world data.</p><p>Over the last year we have been applying some of these new computational scaling methods to Bayesian phylogenetics. Based on preliminary work from a decade ago, we have developed a new method for accurately estimating the distribution of phylogenetic trees of a phylogenetic analysis (as in the Bayesian context the outcome is not a single estimate, but a distribution). This new method provides better probability estimates and more accurate summary trees compared to previous state-of-the-art techniques. The respective paper will soon appear in print and the methods are integrated within the BEAST2 software (<a href="https://www.beast2.org/">https://www.beast2.org/</a>), now made available for other researchers and industry to use. Our work significantly improves a longstanding method, and we expect the new summary tree method to positively impact hundreds of phylogenetic research projects per year.</p><p>We have further investigated fundamental and longstanding challenges in Bayesian phylogenetics. This has led to the development of new methods (i) to assess the convergence of the inference algorithms (Markov Chain Monte Carlo, MCMC) in terms of the trees, (ii) to detect so-called rogue taxa (main sources of noise), and (iii) to statistically correctly join results of subproblems of challenging datasets with long sequence data. These methods increase the capabilities of research communities in assessing and handling challenging datasets. Furthermore, we have discovered how to compute key statistics for phylogenetic tree distributions. More precisely, we are confident that we can now estimate the effective sample size of a sample from MCMC and we can now compute credible sets of tree distributions, both challenges in high-dimensional combinatorial spaces. These statistics allow better experimental validation of methods, models, and datasets. We are now working on further validation of these new methods experimentally.</p><p><strong>Reducing the pain of ingesting diverse, messy data for our data science methods to use</strong></p><p><strong>(meta-data science)</strong></p><p>Most of the human effort employed in data science is focussed on data ingest activities, including data cleaning, building structured data tables and ensuring that the data models created are logical and easy to maintain.</p><p>In meta data science, we have continued to advance three lines of research this year: (1) the discovery of data dependencies, (2) the design of data spaces, and (3) the management of data integrity. The contributions directly address boundaries of modern data management in classical relational databases as well as modern graph databases. They affect update and query operations in every implementation of databases in these models, with relational databases being still the most used databases and graph databases emerging as a new modern standard.</p><p>We have made two main contributions towards (1). Firstly, we have run a demonstration to international colleagues of our Entity/Relationship Profiling tool at the 40<sup>th</sup> IEEE International Conference on Data Engineering. The tool automatically recommends keys and foreign keys it mines from database tables, ranks these according to various metrics and guides the user by smart data exemplars towards identifying those keys and foreign keys that are meaningful for the user. The tool has capabilities beyond any of those available in commercial products and beyond those previously developed in academia. Secondly, we have developed the first approach towards mining meaningful keys from incomplete and inconsistent database tables that is based on well-defined measures, capable of separating those keys that hold accidentally and those that are sensible for the underlying application domain [2]. Based on certain input parameters, the miner can explore a trade-off between the precision and recall of meaningful keys, ranging through the full spectrum from high recall and low precision to high precision and low recall. Algorithms have been fully analysed, implemented and tested.</p><p>In addressing (2), we have developed the first normalization approach of property graphs [3], extending classical definitions, achievements, and normalization algorithms for Boyce-Codd and Third Normal Forms from relational to graph databases. This was very well-received and resulted in an invitation to a special journal issue for the ‘Best of VLDB 2023’ [4]. In [5], we have introduced the concept of Entity/Relationship graphs, as a means to unify classical conceptual and logical data modelling with modern graph modelling and showcasing that the use of Entity/Relationship graphs eliminates the need for attribute redundancy, taking the maintenance of keys and foreign keys to new levels of efficiency. In addition, we have continued optimizing classical relational database normalization algorithms, culminating in the synthesis of third normal form database schemata that optimize integrity maintenance and update overheads based on the numbers and sizes of minimal keys and functional dependencies [6].</p><p>Finally, we have refined the classical theoretical concept of minimal-reduced and optimal covers for sets of functional dependencies, which is extensively used throughout database design, query evaluation and update operations. For these notions of covers, we separate the functional dependencies into minimal keys and non-key functional dependencies, resulting in mixed covers. Minimal keys result in efficient index structures that eliminate data redundancy and sources of inconsistency, therefore speeding up database operations. Hence, mixed covers optimize data access and the efficiency of database operations [7,8].</p><p><strong>2.  Partnerships</strong> </p><p><strong>3.  Research Outputs</strong></p><p>M. Gahegan, G. Ellerm, B. Adams &amp; N. Drost, 2024. “Magically” creating and updating research articles from experiments. <em>eResearch-NZ</em>, Wellington, New Zealand.</p>

<ul><li>

<p>In the top 25% of all research outputs scored by Altmetric</p>

</li><li>

<p>High Attention Score compared to outputs of the same age (91st percentile)</p>

</li><li>

<p>Associated code and datasets are available from:</p>

<ul><li>

<p><a href="https://github.com/Gardner-BinfLab/PCPBSlim/">https://github.com/Gardner-BinfLab/PCPBSlim/</a> (a slim repository of tool scores &amp; control datasets)</p>

</li><li>

<p><a href="https://github.com/Gardner-BinfLab/PCPBFull/">https://github.com/Gardner-BinfLab/PCPBFull/</a> (the complete set of scripts and data)</p>

</li></ul></li></ul>

<p>Wilcox, P. Summer Internship of iNdigenous peoples in Genomics (SING) Global meeting, May 2024, Vancouver, British Columbia, 2024.</p><p>KOEHLER, H.; LINK: S.: `Entity-Relationship Profiling’, In: Proceedings of the 40<sup>th</sup> IEEE International Conference on Data Engineering (ICDE), pages 5393-5396, 13-16 May, Utrecht, The Netherlands, <a href="https://doi.org/10.1109/ICDE60146.2024.00411">https://doi.org/10.1109/ICDE60146.2024.00411</a>; CORE: A*, 2024.</p><p>KOEHLER, H.; LINK, S.: ‘Charting the Trade-off between Precision and Recall in the Mining of Meaningful Database Keys from Incomplete and Inconsistent Relations’, submitted.</p><p>SKAVANTZOS, P.; LINK, S.: `Normalizing Property Graphs’, In: Proceedings of the VLDB Endowment 16(11): 3031-3043, <a href="https://www.vldb.org/pvldb/vol16/p3031-link.pdf">https://www.vldb.org/pvldb/vol16/p3031-link.pdf</a>; CORE: A*, 2024.</p><p>SKAVANTZOS, P.; LINK, S.: `Entity/Relationship Graphs: Unifying Data Modelling and Taking Integrity Management to the Next Level’, submitted.</p><p>ZHANG, Z.; LINK, S.: `Synthesizing Third Normal Form Schemata that Minimize Integrity Maintenance and Update Overheads’, submitted.</p><p>ZHANG, Z.; LINK, S.: `Mixed Covers: Optimizing Updates and Queries Using Minimal Keys and Functional Dependencies’, submitted to the VLDB Journal (A*).</p><p><strong>4.  training / education / skills dev</strong></p>

<ol>
  <li>Anything else of likely interest to MBIE, tell me that too!</li>
</ol>